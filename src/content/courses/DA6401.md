---
title: "Introduction to Deep Learning"
courseNumber: "DA6401"
description: "A comprehensive introduction to Deep Learning covering neural networks fundamentals, CNNs, RNNs, Transformers, and Generative Models"
semester: spring
year: 2026
schedule: "C Slot: Monday 10:00 am - 10:50 am, Tuesday 09:00 am - 09:50 am, Wednesday 08:00 am - 08:50 am, Friday 12:00 pm - 12:50 pm"
location: "CRC302 & CRC103, IIT Madras"
instructors:
  - ganapathy-krishnamurthi
credits: 12
level: graduate
prerequisites:
  - DA5000 - Mathematical Foundations of Datascience
  - DA5400 - Foundations of Machine Learning
  - Proficiency in Python programming
# syllabusUrl: ""
handoutUrl: "https://drive.google.com/file/d/1aagNyr2yKj7w_jmn_XQ1qdO0Tlj3LQnU/view?usp=sharing"
currentlyOffered: true
draft: false
semesterDates: "January 19, 2026 – May 5, 2026 (Last Instruction Day)"
communication: "Moodle will be the primary mode of communication for announcements. Students are expected to check Moodle regularly."
lectureMode: "All lectures will be conducted in person. When an in-person class is not feasible, online lectures will be held and advance notice shall be provided with details. Missed classes (by the lecturer) will be compensated suitably."
---

## Course Description

This course provides a comprehensive introduction to Deep Learning. Beginning with the history of neural networks and the fundamental Perceptron, we move to modern Feedforward Neural Networks, training methodologies, and optimization techniques. The course covers specialized architectures including Convolutional Neural Networks (CNNs) for computer vision, Recurrent Neural Networks (RNNs) for sequence modeling, and Transformers for natural language processing. We conclude with advanced generative models such as GANs and Diffusion models.

## Grading Scheme & Key Dates

| Component                    | Weight | Date/Deadlines          |
| ---------------------------- | ------ | ----------------------- |
| **Quiz 1 (MCQ)**             | 20%    | **February 18, 2026**   |
| **Quiz 2 (MCQ)**             | 20%    | **March 25, 2026**      |
| **Final Exam (Handwritten)** | 40%    | **May 8, 2026**         |
| **Assignments (4 × 5%)**     | 20%    | See Assignment Schedule |

## Detailed Course Content

### Module 1: Introduction & Fundamentals

- Overview of Deep Learning: History, evolution, and success stories
- Neural Fundamentals: McCulloch-Pitts Neuron, Threshold Logic
- Perceptrons: Architecture, Learning Algorithm, and Limitations (XOR problem)
- Multilayer Perceptrons (MLPs): Hidden layers, non-linearity, and Representation Power

### Module 2: Feedforward Networks & Optimization

- Computations: Sigmoid neurons, Automatic differentiation, Backpropagation derivation
- Optimization: Gradient Descent (GD), Stochastic GD, Momentum, Nesterov, AdaGrad, RMSProp, and Adam
- Initialization: Xavier and He initialization

### Module 3: Regularization & Training Methodology

- Bias-Variance Tradeoff, L2 Regularization, Early Stopping
- Dataset Augmentation, Dropout, Injecting Noise
- Practical Issues: Handling Class Imbalance, Domain Generalization

### Module 4: Convolutional Neural Networks (CNNs)

- Fundamentals: Image Processing, Convolution, stride, padding, pooling
- Architectures: LeNet, AlexNet, VGGNet, GoogLeNet, ResNet
- Advanced Training: Batch Normalization, Improved Activation Functions
- Applications: Object Detection (R-CNN family, YOLO, SSD), Semantic Segmentation (U-Net)

### Module 5: NLP & Sequence Models

- Seq2Seq Tasks, Need for a different architecture
- Embeddings: Word2Vec (Skip-gram/CBOW), Vector representations
- RNNs: Backpropagation Through Time (BPTT), Vanishing/Exploding gradients
- Gated Units: LSTMs and GRUs
- Seq2Seq: Encoder-Decoder models (Overview), Introduction to Attention mechanisms

### Module 6: Transformers & Generative Models

- Transformers: Self-Attention (Multi-headed), Cross Attention, Positional Encoding
- Modern Tokenization (BPE), Vision Transformers (ViT)
- Generative Models: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs)
- Diffusion Models & Introduction to Large Language Models (LLMs)

## Lecture Materials

| Module # | Topic                                                      | Materials                                                                                    | Additional Resources |
| :------- | :--------------------------------------------------------- | :------------------------------------------------------------------------------------------- | :------------------- |
| 1A       | Introduction to Deep Learning and Artificial Neuron Models | [Slides](https://drive.google.com/file/d/1DlMjdGgIejlFrIYCVi6_CC_che-W5ndt/view?usp=sharing) |                      |
| 1B       | Modern Neuron & MLPs                                       | [Slides](https://drive.google.com/file/d/1KQgoS46xPPATmOq-NxnqonIvJYzE99ao/view?usp=sharing) |                      |

## Programming Assignments

Assignments will be implemented in Python (NumPy/PyTorch). They are designed to complement the theoretical material without conflicting with quiz weeks.

| Assignment | Topic | Description | Release Date | Due Date |
| :--------- | :---- | :---------- | :----------- | :------- |

| [**Assignment 1**](https://drive.google.com/file/d/1-nAkKCxiOONQ92ZIDIPctLHOPJ9PfgiU/view?usp=sharing) | Multi-Layer Perceptron for Image Classification | Implement a MLP and backpropagation from scratch using Numpy to classify MNIST digits. | Feb 09, 2026 10:00 AM | Mar 01, 2026 11:59 PM |

<!-- | **Assignment 2** | CNNs & Parametric Optimization (PyTorch) | Train CNNs on CIFAR-10. Compare optimizers (SGD vs. Adam) and regularization techniques.       | Feb 23, 2026 | Mar 13, 2026 |
| **Assignment 3** | Sequence Models (RNN/LSTM)               | Build a Sequence-to-Sequence model (Encoder-Decoder) for basic translation or text generation. | Mar 20, 2026 | Apr 10, 2026 |
| **Assignment 4** | Generative Models (GAN/ViT)              | Implementation of a DCGAN for image generation or a Vision Transformer.                        | Apr 15, 2026 | May 4, 2026  | -->

## References

- **Deep Learning**, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press
- **Dive into Deep Learning**, Zhang et al. (Available online: [d2l.ai](https://d2l.ai))
